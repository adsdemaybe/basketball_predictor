{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import vstack\n",
    "from pandas import read_csv, concat\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, ReLU, Sigmoid, BatchNorm1d\n",
    "from torch.nn import Module\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn.init import xavier_normal_\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.width', None)  \n",
    "pd.set_option('display.max_colwidth', None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    @staticmethod\n",
    "    def flip_data(df):\n",
    "        win_df = df.copy()\n",
    "        lose_df = df.copy()        \n",
    "        win_df = win_df.rename(columns={\n",
    "                'WTeamID': 'CurrentTeamID',\n",
    "                'LTeamID': 'OpponentTeamID',\n",
    "                'WScore': 'CurrentTeam_Score',\n",
    "                'LScore': 'OpponentTeam_Score',\n",
    "                'WFGM': 'CurrentTeam_FGM',\n",
    "                'WFGA': 'CurrentTeam_FGA',\n",
    "                'WFGM': 'CurrentTeam_FGM',\n",
    "                'WFGA': 'CurrentTeam_FGA',\n",
    "                'WFGM3': 'CurrentTeam_FGM3',\n",
    "                'WFGA3': 'CurrentTeam_FGA3',\n",
    "                'WFTM': 'CurrentTeam_FTM',\n",
    "                'WFTA': 'CurrentTeam_FTA',\n",
    "                'WOR': 'CurrentTeam_OR',\n",
    "                'WDR': 'CurrentTeam_DR',\n",
    "                'WAst': 'CurrentTeam_Ast',\n",
    "                'WTO': 'CurrentTeam_TO',\n",
    "                'WStl': 'CurrentTeam_Stl',\n",
    "                'WBlk': 'CurrentTeam_Blk',\n",
    "                'WPF': 'CurrentTeam_PF',\n",
    "                'LFGM': 'OpponentTeam_FGM',\n",
    "                'LFGA': 'OpponentTeam_FGA',\n",
    "                'LFGM3': 'OpponentTeam_FGM3',\n",
    "                'LFGA3': 'OpponentTeam_FGA3',\n",
    "                'LFTM': 'OpponentTeam_FTM',\n",
    "                'LFTA': 'OpponentTeam_FTA',\n",
    "                'LOR': 'OpponentTeam_OR',\n",
    "                'LDR': 'OpponentTeam_DR',\n",
    "                'LAst': 'OpponentTeam_Ast',\n",
    "                'LTO': 'OpponentTeam_TO',\n",
    "                'LStl': 'OpponentTeam_Stl',\n",
    "                'LBlk': 'OpponentTeam_Blk',\n",
    "                'LPF': 'OpponentTeam_PF',\n",
    "                'WScore': 'CurrentTeam_Score',\n",
    "                'LScore': 'OpponentTeam_Score',\n",
    "        })\n",
    "        win_df['Result'] = 1\n",
    "        win_df['CurrentTeam_Loc'] = win_df['WLoc'].map({'H': 1, 'A': -1, 'N': 0})\n",
    "\n",
    "        lose_df = lose_df.rename(columns={\n",
    "                'LTeamID': 'CurrentTeamID',\n",
    "                'WTeamID': 'OpponentTeamID',\n",
    "                'LScore': 'CurrentTeam_Score',\n",
    "                'WScore': 'OpponentTeam_Score',\n",
    "                'LFGM': 'CurrentTeam_FGM',\n",
    "                'LFGA': 'CurrentTeam_FGA',\n",
    "                'LFGM': 'CurrentTeam_FGM',\n",
    "                'LFGA': 'CurrentTeam_FGA',\n",
    "                'LFGM3': 'CurrentTeam_FGM3',\n",
    "                'LFGA3': 'CurrentTeam_FGA3',\n",
    "                'LFTM': 'CurrentTeam_FTM',\n",
    "                'LFTA': 'CurrentTeam_FTA',\n",
    "                'LOR': 'CurrentTeam_OR',\n",
    "                'LDR': 'CurrentTeam_DR',\n",
    "                'LAst': 'CurrentTeam_Ast',\n",
    "                'LTO': 'CurrentTeam_TO',\n",
    "                'LStl': 'CurrentTeam_Stl',\n",
    "                'LBlk': 'CurrentTeam_Blk',\n",
    "                'LPF': 'CurrentTeam_PF',\n",
    "                'WFGM': 'OpponentTeam_FGM',\n",
    "                'WFGA': 'OpponentTeam_FGA',\n",
    "                'WFGM3': 'OpponentTeam_FGM3',\n",
    "                'WFGA3': 'OpponentTeam_FGA3',\n",
    "                'WFTM': 'OpponentTeam_FTM',\n",
    "                'WFTA': 'OpponentTeam_FTA',\n",
    "                'WOR': 'OpponentTeam_OR',\n",
    "                'WDR': 'OpponentTeam_DR',\n",
    "                'WAst': 'OpponentTeam_Ast',\n",
    "                'WTO': 'OpponentTeam_TO',\n",
    "                'WStl': 'OpponentTeam_Stl',\n",
    "                'WBlk': 'OpponentTeam_Blk',\n",
    "                'WPF': 'OpponentTeam_PF',\n",
    "                'LScore': 'CurrentTeam_Score',\n",
    "                'WScore': 'OpponentTeam_Score',\n",
    "        })\n",
    "        lose_df['Result'] = 0\n",
    "        lose_df['CurrentTeam_Loc'] = lose_df['WLoc'].map({'H': -1, 'A': 1, 'N': 0})\n",
    "\n",
    "        combined = pd.concat([win_df, lose_df], ignore_index=True)\n",
    "        return combined.drop(columns=['WLoc'])\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_percents(df):\n",
    "        # Current team percentages\n",
    "        df['CurrentTeam_FG_Pct'] = df['CurrentTeam_FGM'] / df['CurrentTeam_FGA']\n",
    "        df['CurrentTeam_3P_Pct'] = df['CurrentTeam_FGM3'] / df['CurrentTeam_FGA3']\n",
    "        df['CurrentTeam_FT_Pct'] = df['CurrentTeam_FTM'] / df['CurrentTeam_FTA']\n",
    "        df['CurrentTeam_Reb'] = df['CurrentTeam_OR'] + df['CurrentTeam_DR']\n",
    "\n",
    "        # Opponent team percentages\n",
    "        df['OpponentTeam_FG_Pct'] = df['OpponentTeam_FGM'] / df['OpponentTeam_FGA']\n",
    "        df['OpponentTeam_3P_Pct'] = df['OpponentTeam_FGM3'] / df['OpponentTeam_FGA3']\n",
    "        df['OpponentTeam_FT_Pct'] = df['OpponentTeam_FTM'] / df['OpponentTeam_FTA']\n",
    "        df['OpponentTeam_Reb'] = df['OpponentTeam_OR'] + df['OpponentTeam_DR']\n",
    "\n",
    "        # Remove redundant columns\n",
    "        redundant = [\n",
    "            'CurrentTeam_FGM', 'CurrentTeam_FGA', 'CurrentTeam_FGM3', 'CurrentTeam_FGA3',\n",
    "            'CurrentTeam_FTM', 'CurrentTeam_FTA', 'CurrentTeam_OR', 'CurrentTeam_DR',\n",
    "            'OpponentTeam_FGM', 'OpponentTeam_FGA', 'OpponentTeam_FGM3', 'OpponentTeam_FGA3',\n",
    "            'OpponentTeam_FTM', 'OpponentTeam_FTA', 'OpponentTeam_OR', 'OpponentTeam_DR'\n",
    "        ]\n",
    "        return df.drop(columns=redundant).fillna(0)\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        df = pd.read_csv(path)\n",
    "        new_order = ['Season', 'DayNum', 'WTeamID','LTeamID', 'WScore', 'LScore', 'WLoc', 'NumOT', \n",
    "                    'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR','WAst', \n",
    "                    'WTO', 'WStl', 'WBlk', 'WPF', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3','LFTM', \n",
    "                    'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF']\n",
    "        df = df[new_order]\n",
    "\n",
    "        most_recent_season = df['Season'].max()\n",
    "        df['delta_time'] = most_recent_season - df['Season']\n",
    "        lambda_decay = 0.5  \n",
    "        df['weight'] = np.exp(-lambda_decay * df['delta_time'])\n",
    "\n",
    "        df = self.flip_data(df)\n",
    "        df = self.calculate_percents(df)\n",
    "        df = df.drop(columns=['Season', 'delta_time', 'weight'], errors='ignore')\n",
    "        self.X = df.drop(columns=['Result']).values.astype(np.float32)\n",
    "        self.y = df['Result'].values.astype(np.float32).reshape(-1, 1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def get_splits(self, test_ratio=0.2):\n",
    "        test_size = int(len(self) * test_ratio)\n",
    "        return random_split(self, [len(self)-test_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):    \n",
    "    def __init__(self, n_inputs):\n",
    "            super(MLP, self).__init__()\n",
    "            \n",
    "            # Layer 1\n",
    "            self.hidden1 = Linear(n_inputs, 20)\n",
    "            kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "            self.bn1 = BatchNorm1d(20)\n",
    "            self.act1 = ReLU()\n",
    "            \n",
    "            # Layer 2\n",
    "            self.hidden2 = Linear(20, 10)\n",
    "            kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "            self.bn2 = BatchNorm1d(10)\n",
    "            self.act2 = ReLU()\n",
    "            \n",
    "            # Layer 3\n",
    "            self.hidden3 = Linear(10, 8)\n",
    "            kaiming_uniform_(self.hidden3.weight, nonlinearity='relu')\n",
    "            self.bn3 = BatchNorm1d(8)\n",
    "            self.act3 = ReLU()\n",
    "            \n",
    "            # Output Layer\n",
    "            self.output = Linear(8, 1)\n",
    "            xavier_normal_(self.output.weight)\n",
    "            self.act4 = Sigmoid()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.act1(self.bn1(self.hidden1(X)))\n",
    "        X = self.act2(self.bn2(self.hidden2(X)))\n",
    "        X = self.act3(self.bn3(self.hidden3(X)))\n",
    "        return self.act4(self.output(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(path, test_size=0.2, batch_size=64):\n",
    "    # Create dataset and split\n",
    "    dataset = CSVDataset(path)\n",
    "    train_size = int((1 - test_size) * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_data, test_data = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dl = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_dl = DataLoader(test_data, batch_size=batch_size)\n",
    "    return train_dl, test_dl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dl, model, epochs=100, lr=0.01, validation_dl=None, device='cpu'):\n",
    "    model = model.to(device)\n",
    "    criterion = BCELoss()\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, verbose=True)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'lr': []}\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_iter = tqdm(train_dl, desc=f'Epoch {epoch+1}/{epochs} [Train]', leave=False)\n",
    "        \n",
    "        for inputs, targets in train_iter:\n",
    "            inputs = inputs.to(device).float()\n",
    "            targets = targets.to(device).float().view(-1, 1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            train_iter.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Validation Phase\n",
    "        val_loss = 0.0\n",
    "        if validation_dl:\n",
    "            model.eval()\n",
    "            val_iter = tqdm(validation_dl, desc=f'Epoch {epoch+1}/{epochs} [Val]', leave=False)\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in val_iter:\n",
    "                    inputs = inputs.to(device).float()\n",
    "                    targets = targets.to(device).float().view(-1, 1)\n",
    "                    outputs = model(inputs)\n",
    "                    val_loss += criterion(outputs, targets).item() * inputs.size(0)\n",
    "\n",
    "            val_loss /= len(validation_dl.dataset)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "        # Update history\n",
    "        train_loss /= len(train_dl.dataset)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        # Epoch summary\n",
    "        log = f'Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f}'\n",
    "        if validation_dl:\n",
    "            log += f' | Val Loss: {val_loss:.4f} | LR: {history[\"lr\"][-1]:.2e}'\n",
    "        print(log)\n",
    "\n",
    "    print('Training complete')\n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(test_dl, model, device='cpu'):\n",
    "    model = model.to(device).eval()\n",
    "    predictions, actuals = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(test_dl, desc='Evaluating'):\n",
    "            inputs = inputs.to(device).float()\n",
    "            targets = targets.to(device).float()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            preds = torch.round(outputs).cpu().numpy()\n",
    "            \n",
    "            predictions.extend(preds.ravel().tolist())\n",
    "            actuals.extend(targets.cpu().numpy().ravel().tolist())\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(actuals, predictions),\n",
    "        'predictions': np.array(predictions),\n",
    "        'actuals': np.array(actuals)\n",
    "    }\n",
    "\n",
    "def predict(row, model, device='cpu'):\n",
    "    model = model.to(device).eval()\n",
    "    with torch.no_grad():\n",
    "        tensor = torch.as_tensor(row, dtype=torch.float32).to(device)\n",
    "        prediction = model(tensor.unsqueeze(0))\n",
    "        return torch.sigmoid(prediction).cpu().numpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 186756, Test samples: 46690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/advaithvecham/Studying Stuff/PoC/.venv/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "                                                             \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x25 and 34x20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dl\u001b[38;5;241m.\u001b[39mdataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_dl\u001b[38;5;241m.\u001b[39mdataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m MLP(\u001b[38;5;241m34\u001b[39m)  \u001b[38;5;66;03m# Match your actual input feature count\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m acc \u001b[38;5;241m=\u001b[39m evaluate_model(test_dl, model)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[151], line 21\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_dl, model, epochs, lr, validation_dl, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Studying Stuff/PoC/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Studying Stuff/PoC/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[149], line 29\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 29\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     30\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden2(X)))\n\u001b[1;32m     31\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden3(X)))\n",
      "File \u001b[0;32m~/Studying Stuff/PoC/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Studying Stuff/PoC/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Studying Stuff/PoC/.venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x25 and 34x20)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    path = \"/Users/advaithvecham/Studying Stuff/PoC/march-machine-learning-mania-2025/MRegularSeasonDetailedResults.csv\"\n",
    "    train_dl, test_dl = prepare_data(path)\n",
    "    print(f\"Training samples: {len(train_dl.dataset)}, Test samples: {len(test_dl.dataset)}\")\n",
    "    \n",
    "    model = MLP(34)  # Match your actual input feature count\n",
    "    train_model(train_dl, model)\n",
    "    \n",
    "    acc = evaluate_model(test_dl, model)\n",
    "    print(f'Accuracy: {acc:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "# Display all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None) \n",
    "\n",
    "df = pd.read_csv(\"march-machine-learning-mania-2025/MNCAATourneyDetailedResults.csv\")\n",
    "new_order = ['Season', 'DayNum', 'WTeamID','LTeamID', 'WScore', 'LScore', 'WLoc',\n",
    "       'NumOT', 'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR',\n",
    "       'WAst', 'WTO', 'WStl', 'WBlk', 'WPF', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3',\n",
    "       'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF']\n",
    "df = df[new_order]\n",
    "\n",
    "       # Define a function to flip the data\n",
    "def flip_data(df):\n",
    "       # Create the winning team's perspective (Y = 1)\n",
    "       win_df = df.copy()\n",
    "       win_df['CurrentTeamID'] = win_df['WTeamID']\n",
    "       win_df['OpponentTeamID'] = win_df['LTeamID']\n",
    "       win_df['Result'] = 1  # Win\n",
    "       win_df['CurrentTeam_Loc'] = win_df['WLoc']\n",
    "\n",
    "       # Create the losing team's perspective (Y = 0)\n",
    "       lose_df = df.copy()\n",
    "       lose_df['CurrentTeamID'] = lose_df['LTeamID']\n",
    "       lose_df['OpponentTeamID'] = lose_df['WTeamID']\n",
    "       lose_df['Result'] = 0  # Loss\n",
    "       \n",
    "       # Flip the location for the losing team's perspective\n",
    "       lose_df['CurrentTeam_Loc'] = lose_df['WLoc'].map({'H': -1, 'A': 1, 'N':0})\n",
    "\n",
    "       # Rename columns for the winning team's perspective\n",
    "       win_df = win_df.rename(columns={\n",
    "              'WFGM': 'CurrentTeam_FGM',\n",
    "              'WFGA': 'CurrentTeam_FGA',\n",
    "              'WFGM3': 'CurrentTeam_FGM3',\n",
    "              'WFGA3': 'CurrentTeam_FGA3',\n",
    "              'WFTM': 'CurrentTeam_FTM',\n",
    "              'WFTA': 'CurrentTeam_FTA',\n",
    "              'WOR': 'CurrentTeam_OR',\n",
    "              'WDR': 'CurrentTeam_DR',\n",
    "              'WAst': 'CurrentTeam_Ast',\n",
    "              'WTO': 'CurrentTeam_TO',\n",
    "              'WStl': 'CurrentTeam_Stl',\n",
    "              'WBlk': 'CurrentTeam_Blk',\n",
    "              'WPF': 'CurrentTeam_PF',\n",
    "              'LFGM': 'OpponentTeam_FGM',\n",
    "              'LFGA': 'OpponentTeam_FGA',\n",
    "              'LFGM3': 'OpponentTeam_FGM3',\n",
    "              'LFGA3': 'OpponentTeam_FGA3',\n",
    "              'LFTM': 'OpponentTeam_FTM',\n",
    "              'LFTA': 'OpponentTeam_FTA',\n",
    "              'LOR': 'OpponentTeam_OR',\n",
    "              'LDR': 'OpponentTeam_DR',\n",
    "              'LAst': 'OpponentTeam_Ast',\n",
    "              'LTO': 'OpponentTeam_TO',\n",
    "              'LStl': 'OpponentTeam_Stl',\n",
    "              'LBlk': 'OpponentTeam_Blk',\n",
    "              'LPF': 'OpponentTeam_PF',\n",
    "              'WScore': 'CurrentTeam_Score',\n",
    "              'LScore': 'OpponentTeam_Score',\n",
    "       })\n",
    "\n",
    "       # Rename columns for the losing team's perspective\n",
    "       lose_df = lose_df.rename(columns={\n",
    "              'LFGM': 'CurrentTeam_FGM',\n",
    "              'LFGA': 'CurrentTeam_FGA',\n",
    "              'LFGM3': 'CurrentTeam_FGM3',\n",
    "              'LFGA3': 'CurrentTeam_FGA3',\n",
    "              'LFTM': 'CurrentTeam_FTM',\n",
    "              'LFTA': 'CurrentTeam_FTA',\n",
    "              'LOR': 'CurrentTeam_OR',\n",
    "              'LDR': 'CurrentTeam_DR',\n",
    "              'LAst': 'CurrentTeam_Ast',\n",
    "              'LTO': 'CurrentTeam_TO',\n",
    "              'LStl': 'CurrentTeam_Stl',\n",
    "              'LBlk': 'CurrentTeam_Blk',\n",
    "              'LPF': 'CurrentTeam_PF',\n",
    "              'WFGM': 'OpponentTeam_FGM',\n",
    "              'WFGA': 'OpponentTeam_FGA',\n",
    "              'WFGM3': 'OpponentTeam_FGM3',\n",
    "              'WFGA3': 'OpponentTeam_FGA3',\n",
    "              'WFTM': 'OpponentTeam_FTM',\n",
    "              'WFTA': 'OpponentTeam_FTA',\n",
    "              'WOR': 'OpponentTeam_OR',\n",
    "              'WDR': 'OpponentTeam_DR',\n",
    "              'WAst': 'OpponentTeam_Ast',\n",
    "              'WTO': 'OpponentTeam_TO',\n",
    "              'WStl': 'OpponentTeam_Stl',\n",
    "              'WBlk': 'OpponentTeam_Blk',\n",
    "              'WPF': 'OpponentTeam_PF',\n",
    "              'LScore': 'CurrentTeam_Score',\n",
    "              'WScore': 'OpponentTeam_Score',\n",
    "       })\n",
    "\n",
    "       # Combine the two DataFrames\n",
    "       flipped_df = pd.concat([win_df, lose_df], ignore_index=True)\n",
    "\n",
    "       # Drop the original 'W' and 'L' columns\n",
    "       flipped_df = flipped_df.drop(columns=['WLoc', 'WTeamID', 'LTeamID'])\n",
    "\n",
    "       return flipped_df\n",
    "\n",
    "def calculate_percents(df):\n",
    "       # Calculate percentages for the current team\n",
    "       df['CurrentTeam_FG_Pct'] = df['CurrentTeam_FGM'] / df['CurrentTeam_FGA']  # Field goal percentage\n",
    "       df['CurrentTeam_3P_Pct'] = df['CurrentTeam_FGM3'] / df['CurrentTeam_FGA3']  # 3-point percentage\n",
    "       df['CurrentTeam_FT_Pct'] = df['CurrentTeam_FTM'] / df['CurrentTeam_FTA']  # Free throw percentage\n",
    "       df['CurrentTeam_Reb'] = df['CurrentTeam_OR'] + df['CurrentTeam_DR']  # Total rebounds\n",
    "\n",
    "       # Calculate percentages for the opponent team\n",
    "       df['OpponentTeam_FG_Pct'] = df['OpponentTeam_FGM'] / df['OpponentTeam_FGA']  # Field goal percentage\n",
    "       df['OpponentTeam_3P_Pct'] = df['OpponentTeam_FGM3'] / df['OpponentTeam_FGA3']  # 3-point percentage\n",
    "       df['OpponentTeam_FT_Pct'] = df['OpponentTeam_FTM'] / df['OpponentTeam_FTA']  # Free throw percentage\n",
    "       df['OpponentTeam_Reb'] = df['OpponentTeam_OR'] + df['OpponentTeam_DR']  # Total rebounds\n",
    "\n",
    "       # Drop redundant raw count columns\n",
    "       redundant_columns = [\n",
    "              'CurrentTeam_FGM', 'CurrentTeam_FGA', 'CurrentTeam_FGM3', 'CurrentTeam_FGA3',\n",
    "              'CurrentTeam_FTM', 'CurrentTeam_FTA', 'CurrentTeam_OR', 'CurrentTeam_DR',\n",
    "              'OpponentTeam_FGM', 'OpponentTeam_FGA', 'OpponentTeam_FGM3', 'OpponentTeam_FGA3',\n",
    "              'OpponentTeam_FTM', 'OpponentTeam_FTA', 'OpponentTeam_OR', 'OpponentTeam_DR'\n",
    "       ]\n",
    "       df = df.drop(columns=redundant_columns)\n",
    "\n",
    "       # Handle division by zero (replace NaN with 0)\n",
    "       df = df.fillna(0)\n",
    "       \n",
    "       return df\n",
    "\n",
    "# Calculate delta_time\n",
    "most_recent_season = df['Season'].max()\n",
    "df['delta_time'] = most_recent_season - df['Season']\n",
    "\n",
    "# Calculate weights using exponential decay\n",
    "lambda_decay = 0.5  # Tune this hyperparameter\n",
    "df['weight'] = np.exp(-lambda_decay * df['delta_time'])\n",
    "\n",
    "# Use the weights as sample weights\n",
    "sample_weights = df['weight'].values\n",
    "\n",
    "df = flip_data(df)\n",
    "\n",
    "# df.pop('weight')\n",
    "# df.pop('delta_time')\n",
    "# df.pop('Season')\n",
    "df = calculate_percents(df)\n",
    "\n",
    "# df.columns\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Module' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMLP\u001b[39;00m(\u001b[43mModule\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# define model elements\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_inputs):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m(MLP, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Module' is not defined"
     ]
    }
   ],
   "source": [
    "# class MLP(Module):\n",
    "#     # define model elements\n",
    "#     def __init__(self, n_inputs):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.layer = Linear(n_inputs, 1)\n",
    "#         self.activation = Sigmoid()\n",
    " \n",
    "#     # forward propagate input\n",
    "#     def forward(self, X):\n",
    "#         X = self.layer(X)\n",
    "#         X = self.activation(X)\n",
    "#         return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
