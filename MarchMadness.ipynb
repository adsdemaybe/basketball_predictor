{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import tensor, float32, cat, mean, std\n",
    "from torch.nn import Linear, ReLU, Sigmoid, BatchNorm1d, Module, MSELoss, BCELoss\n",
    "from torch.optim import Adam, SGD, RMSprop\n",
    "from torch.utils.data import random_split, Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.init import xavier_normal_\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class that processes a CSV file containing basketball match data, \n",
    "    transforms it into a structured format, applies feature engineering, and prepares \n",
    "    it for model training.\n",
    "\n",
    "    This dataset supports flipping data to create balanced training examples, computing \n",
    "    percentage-based statistics, applying sample weighting, and normalizing features.\n",
    "\n",
    "    Args:\n",
    "        team_id (Series): Team IDs corresponding to each row in the dataset.\n",
    "        X_dataframe (DataFrame): Processed feature data in Pandas DataFrame format.\n",
    "        y_dataframe (Series): Binary outcome labels (win/loss).\n",
    "        column_names (Index): Feature column names.\n",
    "        X (Tensor): Normalized feature tensor.\n",
    "        y (Tensor): Label tensor.\n",
    "        sample_weights (Tensor): Sample weights for training.\n",
    "        mean (Tensor): Mean of each feature for normalization.\n",
    "        std (Tensor): Standard deviation of each feature for normalization.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def flip_data(df):\n",
    "        \"\"\"\n",
    "        Converts a dataset of basketball match results into a balanced dataset \n",
    "        by flipping team perspectives. \n",
    "\n",
    "        For each game, this function creates two entries: one where the winning \n",
    "        team is treated as the \"Current Team\" and one where the losing team is \n",
    "        treated as the \"Current Team.\" It renames columns accordingly to maintain \n",
    "        a consistent format and assigns a binary label (1 for wins, 0 for losses).\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): Raw match data containing team statistics.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: Transformed dataset with flipped perspectives.\n",
    "        \"\"\"\n",
    "        win_df = df.copy()\n",
    "        lose_df = df.copy()\n",
    "        win_df = win_df.rename(columns={\n",
    "            'WTeamID': 'CurrentTeamID',\n",
    "            'LTeamID': 'OpponentTeamID',\n",
    "            'WScore': 'CurrentTeam_Score',\n",
    "            'LScore': 'OpponentTeam_Score',\n",
    "            'WFGM': 'CurrentTeam_FGM',\n",
    "            'WFGA': 'CurrentTeam_FGA',\n",
    "            'WFGM': 'CurrentTeam_FGM',\n",
    "            'WFGA': 'CurrentTeam_FGA',\n",
    "            'WFGM3': 'CurrentTeam_FGM3',\n",
    "            'WFGA3': 'CurrentTeam_FGA3',\n",
    "            'WFTM': 'CurrentTeam_FTM',\n",
    "            'WFTA': 'CurrentTeam_FTA',\n",
    "            'WOR': 'CurrentTeam_OR',\n",
    "            'WDR': 'CurrentTeam_DR',\n",
    "            'WAst': 'CurrentTeam_Ast',\n",
    "            'WTO': 'CurrentTeam_TO',\n",
    "            'WStl': 'CurrentTeam_Stl',\n",
    "            'WBlk': 'CurrentTeam_Blk',\n",
    "            'WPF': 'CurrentTeam_PF',\n",
    "            'LFGM': 'OpponentTeam_FGM',\n",
    "            'LFGA': 'OpponentTeam_FGA',\n",
    "            'LFGM3': 'OpponentTeam_FGM3',\n",
    "            'LFGA3': 'OpponentTeam_FGA3',\n",
    "            'LFTM': 'OpponentTeam_FTM',\n",
    "            'LFTA': 'OpponentTeam_FTA',\n",
    "            'LOR': 'OpponentTeam_OR',\n",
    "            'LDR': 'OpponentTeam_DR',\n",
    "            'LAst': 'OpponentTeam_Ast',\n",
    "            'LTO': 'OpponentTeam_TO',\n",
    "            'LStl': 'OpponentTeam_Stl',\n",
    "            'LBlk': 'OpponentTeam_Blk',\n",
    "            'LPF': 'OpponentTeam_PF',\n",
    "            'WScore': 'CurrentTeam_Score',\n",
    "            'LScore': 'OpponentTeam_Score',\n",
    "        })\n",
    "        win_df['Result'] = 1\n",
    "        win_df['CurrentTeam_Loc'] = win_df['WLoc'].map({'H': 1, 'A': -1, 'N': 0})\n",
    "                \n",
    "        lose_df = lose_df.rename(columns={\n",
    "            'LTeamID': 'CurrentTeamID',\n",
    "            'WTeamID': 'OpponentTeamID',\n",
    "            'LScore': 'CurrentTeam_Score',\n",
    "            'WScore': 'OpponentTeam_Score',\n",
    "            'LFGM': 'CurrentTeam_FGM',\n",
    "            'LFGA': 'CurrentTeam_FGA',\n",
    "            'LFGM': 'CurrentTeam_FGM',\n",
    "            'LFGA': 'CurrentTeam_FGA',\n",
    "            'LFGM3': 'CurrentTeam_FGM3',\n",
    "            'LFGA3': 'CurrentTeam_FGA3',\n",
    "            'LFTM': 'CurrentTeam_FTM',\n",
    "            'LFTA': 'CurrentTeam_FTA',\n",
    "            'LOR': 'CurrentTeam_OR',\n",
    "            'LDR': 'CurrentTeam_DR',\n",
    "            'LAst': 'CurrentTeam_Ast',\n",
    "            'LTO': 'CurrentTeam_TO',\n",
    "            'LStl': 'CurrentTeam_Stl',\n",
    "            'LBlk': 'CurrentTeam_Blk',\n",
    "            'LPF': 'CurrentTeam_PF',\n",
    "            'WFGM': 'OpponentTeam_FGM',\n",
    "            'WFGA': 'OpponentTeam_FGA',\n",
    "            'WFGM3': 'OpponentTeam_FGM3',\n",
    "            'WFGA3': 'OpponentTeam_FGA3',\n",
    "            'WFTM': 'OpponentTeam_FTM',\n",
    "            'WFTA': 'OpponentTeam_FTA',\n",
    "            'WOR': 'OpponentTeam_OR',\n",
    "            'WDR': 'OpponentTeam_DR',\n",
    "            'WAst': 'OpponentTeam_Ast',\n",
    "            'WTO': 'OpponentTeam_TO',\n",
    "            'WStl': 'OpponentTeam_Stl',\n",
    "            'WBlk': 'OpponentTeam_Blk',\n",
    "            'WPF': 'OpponentTeam_PF',\n",
    "            'LScore': 'CurrentTeam_Score',\n",
    "            'WScore': 'OpponentTeam_Score',\n",
    "        })\n",
    "        lose_df['Result'] = 0\n",
    "        lose_df['CurrentTeam_Loc'] = lose_df['WLoc'].map({'H': -1, 'A': 1, 'N': 0})\n",
    "        \n",
    "        combined = pd.concat([win_df, lose_df], ignore_index=True)\n",
    "        return combined.drop(columns=['WLoc'])\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_percents(df):\n",
    "        \"\"\"\n",
    "        Computes percentage-based performance metrics for teams in each game.\n",
    "\n",
    "        This function calculates field goal, three-point, and free throw \n",
    "        percentages, as well as total rebounds, for both the current and \n",
    "        opponent teams. It then removes redundant raw statistics to reduce \n",
    "        dimensionality and fills missing values with zero.\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): Transformed dataset with raw statistics.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: Dataset with percentage-based metrics replacing raw stats.\n",
    "        \"\"\"\n",
    "        df['CurrentTeam_FG_Pct'] = df['CurrentTeam_FGM'] / df['CurrentTeam_FGA']\n",
    "        df['CurrentTeam_3P_Pct'] = df['CurrentTeam_FGM3'] / df['CurrentTeam_FGA3']\n",
    "        df['CurrentTeam_FT_Pct'] = df['CurrentTeam_FTM'] / df['CurrentTeam_FTA']\n",
    "        df['CurrentTeam_Reb'] = df['CurrentTeam_OR'] + df['CurrentTeam_DR']\n",
    "\n",
    "        df['OpponentTeam_FG_Pct'] = df['OpponentTeam_FGM'] / df['OpponentTeam_FGA']\n",
    "        df['OpponentTeam_3P_Pct'] = df['OpponentTeam_FGM3'] / df['OpponentTeam_FGA3']\n",
    "        df['OpponentTeam_FT_Pct'] = df['OpponentTeam_FTM'] / df['OpponentTeam_FTA']\n",
    "        df['OpponentTeam_Reb'] = df['OpponentTeam_OR'] + df['OpponentTeam_DR']\n",
    "\n",
    "        redundant = [\n",
    "            'CurrentTeam_FGM', 'CurrentTeam_FGA', 'CurrentTeam_FGM3', 'CurrentTeam_FGA3',\n",
    "            'CurrentTeam_FTM', 'CurrentTeam_FTA', 'CurrentTeam_OR', 'CurrentTeam_DR',\n",
    "            'OpponentTeam_FGM', 'OpponentTeam_FGA', 'OpponentTeam_FGM3', 'OpponentTeam_FGA3',\n",
    "            'OpponentTeam_FTM', 'OpponentTeam_FTA', 'OpponentTeam_OR', 'OpponentTeam_DR'\n",
    "        ]\n",
    "        return df.drop(columns=redundant).fillna(0)\n",
    "        \n",
    "    def __init__(self, path, lambda_decay=0.5):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by loading, processing, and normalizing match data.\n",
    "\n",
    "        This constructor loads the dataset from a CSV file, flips data to create \n",
    "        balanced training examples, calculates derived statistics, applies sample \n",
    "        weighting based on season recency, normalizes features, and prepares tensors \n",
    "        for PyTorch models.\n",
    "\n",
    "        Args:\n",
    "            path (str): Path to the CSV file containing match data.\n",
    "            lambda_decay (float, optional): Decay rate for sample weighting based \n",
    "                on season recency. Defaults to 0.5.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(path)\n",
    "        most_recent_season = df['Season'].max()\n",
    "        df = self.flip_data(df)\n",
    "        \n",
    "        df['SampleWeight'] = np.exp(-lambda_decay * (most_recent_season - df['Season']))\n",
    "        df['SampleWeight'] = df['SampleWeight'] / df['SampleWeight'].sum() \n",
    "        \n",
    "        df = self.calculate_percents(df)\n",
    "        \n",
    "        df = df.drop(columns=['Season'])\n",
    "        \n",
    "        self.team_id = df['CurrentTeamID']\n",
    "        self.X_dataframe = df.drop(columns=['Result', 'DayNum', 'CurrentTeamID', 'OpponentTeamID', 'CurrentTeam_Score', 'OpponentTeam_Score', 'SampleWeight'])\n",
    "        self.y_dataframe = df['Result']\n",
    "        \n",
    "        self.column_names = self.X_dataframe.columns\n",
    "        \n",
    "        self.X = tensor(self.X_dataframe.values, dtype=float32)\n",
    "        self.y = tensor(self.y_dataframe.values, dtype=float32).reshape(-1, 1)\n",
    "        self.sample_weights = tensor(df['SampleWeight'].values, dtype=float32).reshape(-1, 1)  \n",
    "        \n",
    "        self.mean = mean(self.X, dim=0)\n",
    "        self.std = std(self.X, dim=0)\n",
    "        self.std[self.std == 0] = 1e-8\n",
    "        self.X = (self.X - self.mean) / self.std\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a specific sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - X[idx] (Tensor): Feature tensor for the given index.\n",
    "                - y[idx] (Tensor): Label tensor (win/loss) for the given index.\n",
    "                - sample_weights[idx] (Tensor): Sample weight tensor for the given index.\n",
    "        \"\"\"\n",
    "        return self.X[idx], self.y[idx], self.sample_weights[idx]\n",
    "    \n",
    "    def get_column_names(self):\n",
    "        \"\"\"\n",
    "        Retrieves the names of the feature columns in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            Index: List of feature column names.\n",
    "        \"\"\"\n",
    "        return self.column_names\n",
    "    \n",
    "    def get_example_data(self):\n",
    "        \"\"\"\n",
    "        Returns a small subset of the dataset for inspection.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The first five rows of the feature dataset.\n",
    "        \"\"\"\n",
    "        return self.X_dataframe[:5]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, BatchNorm1d, LeakyReLU, Dropout\n",
    "from torch.nn.init import xavier_normal_, kaiming_uniform_\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_inputs, dropout_value=0.2, hidden_units=[20, 10, 8], relu_slope=0.01):\n",
    "        \"\"\"\n",
    "        Initializes a multi-layer perceptron (MLP) model with configurable hidden layers, batch normalization, \n",
    "        dropout, and LeakyReLU activation.\n",
    "\n",
    "        This constructor dynamically creates a sequence of fully connected layers (hidden layers) based on \n",
    "        the `hidden_units` parameter. Each hidden layer is followed by batch normalization, a LeakyReLU \n",
    "        activation function with a specified slope, and a dropout layer to improve generalization.\n",
    "\n",
    "        The final output layer is a single neuron initialized using Xavier normalization, designed for \n",
    "        binary classification or regression tasks.\n",
    "\n",
    "        Args:\n",
    "            n_inputs (int): Number of input features.\n",
    "            dropout_value (float, optional): Dropout rate applied after each hidden layer. Defaults to 0.2.\n",
    "            hidden_units (list, optional): List defining the number of neurons in each hidden layer. Defaults to [20, 10, 8].\n",
    "            relu_slope (float, optional): Negative slope for the LeakyReLU activation function. Defaults to 0.01.\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.model_params = nn.ModuleDict()\n",
    "\n",
    "        # Build hidden layers dynamically\n",
    "        for i, hidden_dim in enumerate(hidden_units):\n",
    "            if i == 0:\n",
    "                self.model_params['hidden0'] = Linear(n_inputs, hidden_dim)\n",
    "                kaiming_uniform_(self.model_params['hidden0'].weight, nonlinearity='leaky_relu')\n",
    "            else:\n",
    "                self.model_params[f'hidden{i}'] = Linear(hidden_units[i-1], hidden_dim)\n",
    "                kaiming_uniform_(self.model_params[f'hidden{i}'].weight, nonlinearity='leaky_relu')\n",
    "            \n",
    "            self.model_params[f'bn{i}'] = BatchNorm1d(hidden_dim)\n",
    "            self.model_params[f'act{i}'] = LeakyReLU(relu_slope)\n",
    "            self.model_params[f'drop{i}'] = Dropout(dropout_value)\n",
    "\n",
    "        self.output = Linear(hidden_units[-1], 1)\n",
    "        xavier_normal_(self.output.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the MLP model.\n",
    "\n",
    "        The input `X` is passed sequentially through the hidden layers, \n",
    "        where each layer consists of a linear transformation followed by \n",
    "        batch normalization, LeakyReLU activation, and dropout. \n",
    "        Finally, the processed tensor is passed through the output layer \n",
    "        to generate the final prediction.\n",
    "\n",
    "        Args:\n",
    "            X (Tensor): Input tensor of shape (batch_size, n_inputs).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, 1), representing \n",
    "            the model's predictions.\n",
    "        \"\"\"\n",
    "        for i in range(len(self.hidden_units)):\n",
    "            X = self.model_params[f'hidden{i}'](X)\n",
    "            X = self.model_params[f'bn{i}'](X)\n",
    "            X = self.model_params[f'act{i}'](X)\n",
    "            X = self.model_params[f'drop{i}'](X)\n",
    "        X = self.output(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(path, test_size=0.2, batch_size=64, get_data_headers = True):\n",
    "    \"\"\"\n",
    "    Loads a dataset from a CSV file, splits it into training and test sets, and returns PyTorch DataLoaders.\n",
    "\n",
    "    This function reads data from a CSV file using the `CSVDataset` class, extracts features, labels, and \n",
    "    sample weights, and then splits the data into training and test sets. The data is converted into PyTorch \n",
    "    tensors and wrapped in `TensorDataset` objects, which are then used to create PyTorch `DataLoader` \n",
    "    instances for efficient mini-batch processing.\n",
    "\n",
    "    Optionally, it can print the column names from the dataset if `get_data_headers` is set to True.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the CSV file containing the dataset.\n",
    "        test_size (float, optional): Proportion of the dataset to be used as the test set. Defaults to 0.2.\n",
    "        batch_size (int, optional): Number of samples per batch for DataLoader. Defaults to 64.\n",
    "        get_data_headers (bool, optional): If True, prints the column names of the dataset. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: \n",
    "            - train_loader (DataLoader): DataLoader for the training set.\n",
    "            - test_loader (DataLoader): DataLoader for the test set.\n",
    "            - num_features (int): Number of features in the dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = CSVDataset(path)\n",
    "    if(get_data_headers):\n",
    "        print(data.get_column_names())\n",
    "\n",
    "    # Ensure sample_weights is correctly split along with X and y\n",
    "    X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(\n",
    "        data.X, data.y, data.sample_weights, train_size=(1 - test_size), shuffle=True, random_state=42\n",
    "    )\n",
    "\n",
    "    # Convert NumPy arrays to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    w_train = torch.tensor(w_train, dtype=torch.float32)\n",
    "\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "    w_test = torch.tensor(w_test, dtype=torch.float32)\n",
    "\n",
    "    # Create TensorDataset\n",
    "    train_dataset = TensorDataset(X_train, y_train, w_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test, w_test)\n",
    "\n",
    "    # Use DataLoader\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "\n",
    "    return train_loader, test_loader, len(data.get_column_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_model(train_loader, test_loader, \n",
    "                n_inputs,  \n",
    "                dropout_value=0.2,\n",
    "                hidden_units=[20, 10, 8, 1],\n",
    "                relu_slope=0.01,\n",
    "                learning_rate=0.001, \n",
    "                epochs=50, \n",
    "                optimizer_type='adam', \n",
    "                loss_fn='bce', \n",
    "                beta_1=0.9, \n",
    "                beta_2=0.999, \n",
    "                verbose=True,\n",
    "                weight_decay = 1e-5):\n",
    "    \"\"\"\n",
    "    Trains a multi-layer perceptron (MLP) model using PyTorch, with configurable hyperparameters.\n",
    "\n",
    "    This function initializes an MLP model with specified hidden layers, activation function slope,\n",
    "    and dropout rate. It trains the model on the provided training dataset using the selected optimizer\n",
    "    (Adam, SGD, or RMSprop) and loss function (Binary Cross-Entropy or Mean Squared Error). The optimizer's\n",
    "    weight_decay parameter applies L2 regularization to prevent overfitting.\n",
    "    \n",
    "    Args:\n",
    "        train_loader (DataLoader): PyTorch DataLoader providing the training data batches.\n",
    "        test_loader (DataLoader): PyTorch DataLoader providing the test data batches.\n",
    "        n_inputs (int): Number of input features in the dataset.\n",
    "        dropout_value (float, optional): Dropout rate applied to hidden layers. Defaults to 0.2.\n",
    "        hidden_units (list, optional): List defining the number of neurons in each hidden layer. Defaults to [20, 10, 8].\n",
    "        relu_slope (float, optional): Negative slope for the LeakyReLU activation function. Defaults to 0.01.\n",
    "        learning_rate (float, optional): Learning rate for the optimizer. Defaults to 0.001.\n",
    "        epochs (int, optional): Number of training epochs. Defaults to 50.\n",
    "        optimizer_type (str, optional): Optimizer choice ('adam', 'sgd', or 'rmsprop'). Defaults to 'adam'.\n",
    "        loss_function (str, optional): Loss function ('bce' for binary classification, 'mse' for regression). Defaults to 'bce'.\n",
    "        regularization (float, optional): (Unused) L2 regularization coefficient. Defaults to 0.01.\n",
    "        beta_1 (float, optional): First momentum term for Adam/RMSprop optimizers. Defaults to 0.9.\n",
    "        beta_2 (float, optional): Second momentum term for Adam optimizer. Defaults to 0.999.\n",
    "        verbose (bool, optional): If True, prints training progress and loss at each epoch. Defaults to True.\n",
    "        weight_decay (float, optional): Weight decay for optimizer regularization. Defaults to 1e-5.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an invalid optimizer type is provided.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A trained PyTorch model and a dictionary containing training and test loss history.\n",
    "    \"\"\"\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        \n",
    "    model = MLP(n_inputs=n_inputs, dropout_value=dropout_value, hidden_units=hidden_units, relu_slope=relu_slope)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    if loss_fn == \"bce\":\n",
    "        loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    else:\n",
    "        loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "    optimizer = {\n",
    "        'adam': Adam(model.parameters(), lr=learning_rate, betas=(beta_1, beta_2), weight_decay=weight_decay),\n",
    "        'sgd': SGD(model.parameters(), lr=learning_rate, momentum=beta_1, weight_decay=weight_decay),\n",
    "        'rmsprop': RMSprop(model.parameters(), lr=learning_rate, alpha=beta_1, weight_decay=weight_decay)\n",
    "    }.get(optimizer_type, None)\n",
    "    if optimizer is None:\n",
    "        raise ValueError(\"Invalid optimizer. Choose 'adam', 'sgd', or 'rmsprop'.\")\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"epoch_time\": [],\n",
    "        \"best_test_loss\": float('inf'),  # Track best validation loss\n",
    "        \"weight_norms\": []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y, batch_w in train_loader:\n",
    "            batch_X, batch_y, batch_w = batch_X.to(device), batch_y.to(device), batch_w.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(batch_X)\n",
    "            loss = loss_fn(y_hat, batch_y)\n",
    "            weighted_loss = loss * batch_w.view(-1, 1)\n",
    "            final_loss = weighted_loss.mean()\n",
    "            final_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += final_loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        history[\"train_loss\"].append(avg_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y, batch_w in test_loader:\n",
    "                batch_X, batch_y, batch_w = batch_X.to(device), batch_y.to(device), batch_w.to(device)\n",
    "                y_hat_dev = model(batch_X)\n",
    "                loss = loss_fn(y_hat_dev, batch_y)\n",
    "                weighted_loss = loss * batch_w.view(-1, 1)\n",
    "                final_loss = weighted_loss.mean()\n",
    "                test_loss += final_loss.item()  # âœ… Corrected accumulation\n",
    "\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        history[\"test_loss\"].append(avg_test_loss)\n",
    "\n",
    "        # Track best validation loss\n",
    "        if avg_test_loss < history[\"best_test_loss\"]:\n",
    "            history[\"best_test_loss\"] = avg_test_loss\n",
    "        \n",
    "        # Debugging: Check that the weights are updating\n",
    "        weight_norm = torch.norm(next(model.parameters())).item()\n",
    "        history[\"weight_norms\"].append(weight_norm)\n",
    "        \n",
    "        # Track time taken for each epoch\n",
    "        epoch_time = time.time() - start_time\n",
    "        history[\"epoch_time\"].append(epoch_time)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "                  f\"- Train Loss: {avg_train_loss:.6f} \"\n",
    "                  f\"- Test Loss: {avg_test_loss:.6f} \"\n",
    "                  f\"- Weight Norm: {weight_norm:.6f} \"\n",
    "                  f\"- Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aggs(dataset, output_file_path='input_data.csv'):\n",
    "    \"\"\"\n",
    "    Computes weighted aggregate statistics for each team based on sample weights and saves the results.\n",
    "\n",
    "    This function processes a dataset by grouping feature values by `TeamID` and calculating \n",
    "    the weighted average for each feature using the provided sample weights. It then constructs \n",
    "    two dictionaries: one containing the aggregated statistics from the team's perspective \n",
    "    (`current_aggs`), and another containing opponent team statistics (`opponent_aggs`). \n",
    "    Additionally, a structured DataFrame is created, where each row represents a team with \n",
    "    separate columns for current and opponent perspectives. The results are saved as a CSV file.\n",
    "\n",
    "    Args:\n",
    "        dataset (CSVDataset): An instance of the `CSVDataset` class containing team feature data.\n",
    "        output_file_path (str, optional): File path to save the aggregated features as a CSV. \n",
    "            Defaults to 'input_data.csv'.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - current_aggs (dict): A dictionary mapping `TeamID` to a NumPy array of aggregated \n",
    "            statistics for the team's perspective.\n",
    "            - opponent_aggs (dict): A dictionary mapping `TeamID` to a NumPy array of aggregated \n",
    "            statistics from the opponent's perspective.\n",
    "    \"\"\"\n",
    "    # Ensure team_id is a pandas Series.\n",
    "    team_ids = dataset.team_id if isinstance(dataset.team_id, pd.Series) else pd.Series(dataset.team_id)\n",
    "    \n",
    "    # Make a copy of the features DataFrame and add the team IDs.\n",
    "    df_features = dataset.X_dataframe.copy()\n",
    "    df_features['TeamID'] = team_ids.values\n",
    "    \n",
    "    # Convert sample_weights to a 1D NumPy array.\n",
    "    if isinstance(dataset.sample_weights, torch.Tensor):\n",
    "        weights = dataset.sample_weights.cpu().detach().numpy().flatten()\n",
    "    else:\n",
    "        weights = np.array(dataset.sample_weights).flatten()\n",
    "    df_features['SampleWeight'] = weights\n",
    "    \n",
    "    # All feature columns are those originally in X_dataframe.\n",
    "    feature_cols = [col for col in df_features.columns if col not in ['TeamID', 'SampleWeight']]\n",
    "    \n",
    "    # Group by TeamID and compute the weighted average for each feature.\n",
    "    aggregates = {}\n",
    "    for team, group in df_features.groupby('TeamID'):\n",
    "        w = group['SampleWeight'].values\n",
    "        avg_features = {}\n",
    "        for col in feature_cols:\n",
    "            avg_features[col] = np.average(group[col].values, weights=w)\n",
    "        aggregates[team] = avg_features\n",
    "        \n",
    "    # Convert the aggregates dictionary to a DataFrame.\n",
    "    agg_df = pd.DataFrame.from_dict(aggregates, orient='index')\n",
    "    agg_df.index.name = 'TeamID'\n",
    "    agg_df.reset_index(inplace=True)\n",
    "    \n",
    "    # Define the orders for current and opponent perspectives.\n",
    "    current_order = ['CurrentTeam_Ast', 'CurrentTeam_TO', 'CurrentTeam_Stl', \n",
    "                    'CurrentTeam_Blk', 'CurrentTeam_PF', 'CurrentTeam_Loc', \n",
    "                    'CurrentTeam_FG_Pct', 'CurrentTeam_3P_Pct', \n",
    "                    'CurrentTeam_FT_Pct', 'CurrentTeam_Reb']\n",
    "    opponent_order = ['OpponentTeam_Ast', 'OpponentTeam_TO', 'OpponentTeam_Stl', \n",
    "                    'OpponentTeam_Blk', 'OpponentTeam_PF', 'OpponentTeam_FG_Pct', \n",
    "                    'OpponentTeam_3P_Pct', 'OpponentTeam_FT_Pct', \n",
    "                    'OpponentTeam_Reb']\n",
    "    \n",
    "    current_aggs = {}\n",
    "    opponent_aggs = {}\n",
    "    combined_rows = []\n",
    "    \n",
    "    for _, row in agg_df.iterrows():\n",
    "        team = int(row['TeamID'])\n",
    "        current_vec = row[current_order].values.astype(np.float32)\n",
    "        opponent_vec = row[opponent_order].values.astype(np.float32)\n",
    "        \n",
    "        current_aggs[team] = current_vec\n",
    "        opponent_aggs[team] = opponent_vec\n",
    "        \n",
    "        combined_row = {'TeamID': team}\n",
    "        for col, val in zip(current_order, current_vec):\n",
    "            combined_row['curr_' + col] = val\n",
    "        for col, val in zip(opponent_order, opponent_vec):\n",
    "            combined_row['opp_' + col] = val\n",
    "        combined_rows.append(combined_row)\n",
    "    \n",
    "    combined_df = pd.DataFrame(combined_rows)\n",
    "    combined_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Aggregated features written to {output_file_path}\")\n",
    "    \n",
    "    return current_aggs, opponent_aggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_data_csv(dataset, output_file_path='input_data.csv'):\n",
    "    \"\"\"\n",
    "    Generates a CSV file containing aggregated statistics for each team based on a given dataset.\n",
    "\n",
    "    This function computes team-level aggregate statistics by calling `compute_aggs`, which calculates \n",
    "    weighted averages for both current and opponent team perspectives. The resulting CSV file contains \n",
    "    one row per team with columns for team-specific and opponent-specific aggregated features. The \n",
    "    function then reads the generated CSV and returns it as a Pandas DataFrame.\n",
    "\n",
    "    The resulting CSV file will have the following structure:\n",
    "      - TeamID: Unique identifier for the team.\n",
    "      - curr_*: Aggregated statistics from the current team's perspective.\n",
    "      - opp_*: Aggregated statistics from the opponent's perspective.\n",
    "\n",
    "    Columns included in the CSV:\n",
    "      - TeamID\n",
    "      - curr_CurrentTeam_Ast, curr_CurrentTeam_TO, curr_CurrentTeam_Stl, curr_CurrentTeam_Blk,\n",
    "        curr_CurrentTeam_PF, curr_CurrentTeam_Loc, curr_CurrentTeam_FG_Pct, curr_CurrentTeam_3P_Pct,\n",
    "        curr_CurrentTeam_FT_Pct, curr_CurrentTeam_Reb\n",
    "      - opp_OpponentTeam_Ast, opp_OpponentTeam_TO, opp_OpponentTeam_Stl, opp_OpponentTeam_Blk,\n",
    "        opp_OpponentTeam_PF, opp_OpponentTeam_FG_Pct, opp_OpponentTeam_3P_Pct, opp_OpponentTeam_FT_Pct,\n",
    "        opp_OpponentTeam_Reb\n",
    "\n",
    "    Args:\n",
    "        dataset (CSVDataset): An instance of the `CSVDataset` class containing match feature data.\n",
    "        output_file_path (str, optional): File path to save the aggregated team statistics. Defaults to 'input_data.csv'.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A Pandas DataFrame containing the aggregated team statistics.\n",
    "    \"\"\"\n",
    "    # Compute aggregates and write the CSV file.\n",
    "    compute_aggs(dataset, output_file_path=output_file_path)\n",
    "    # Read the generated CSV file and return it.\n",
    "    df = pd.read_csv(output_file_path)\n",
    "    print(f\"Input data CSV read from {output_file_path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_aggs(submission_key, model, current_aggs, opponent_aggs, dataset, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generates a probability prediction for a match using aggregated team features.\n",
    "\n",
    "    This function takes a submission key that represents a match-up between two teams, retrieves \n",
    "    their aggregated statistics, normalizes the feature vector using the dataset's stored mean \n",
    "    and standard deviation, and passes it through the trained model to predict the probability \n",
    "    that team A wins against team B.\n",
    "\n",
    "    Args:\n",
    "        submission_key (str): A string in the format \"year_teamidA_teamidB\" representing the match-up.\n",
    "        model (torch.nn.Module): The trained PyTorch model used for prediction.\n",
    "        current_aggs (dict): Dictionary mapping `TeamID` to aggregated statistics from the current team's perspective.\n",
    "        opponent_aggs (dict): Dictionary mapping `TeamID` to aggregated statistics from the opponent's perspective.\n",
    "        dataset (CSVDataset): The dataset instance containing feature normalization parameters.\n",
    "        device (str, optional): The device ('cpu' or 'cuda') on which the prediction should be performed. Defaults to 'cpu'.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the submission key format is incorrect.\n",
    "        ValueError: If the constructed feature vector does not match the expected length.\n",
    "\n",
    "    Returns:\n",
    "        float: The predicted probability that team A wins against team B.\n",
    "    \"\"\"\n",
    "    # Parse the submission key.\n",
    "    parts = submission_key.split('_')\n",
    "    if len(parts) != 3:\n",
    "        raise ValueError(\"Submission key must be in the format 'year_teamidA_teamidB'.\")\n",
    "    _, teamA_str, teamB_str = parts\n",
    "    teamA = int(teamA_str)\n",
    "    teamB = int(teamB_str)\n",
    "    \n",
    "    # For game-level features, set NumOT to 0.\n",
    "    numot = 0.0\n",
    "    \n",
    "    # Look up the aggregated features.\n",
    "    # If a team does not have history, default to zeros.\n",
    "    current_features = current_aggs.get(teamA, np.zeros(10, dtype=np.float32))\n",
    "    opponent_features = opponent_aggs.get(teamB, np.zeros(9, dtype=np.float32))\n",
    "    \n",
    "    # Build the submission feature vector.\n",
    "    # Training feature order: [NumOT] + current team features (10 values) + opponent team features (9 values)\n",
    "    submission_features = np.concatenate([[numot], current_features, opponent_features])\n",
    "    \n",
    "    expected_length = dataset.X_dataframe.shape[1]  # Should be 20 features.\n",
    "    if len(submission_features) != expected_length:\n",
    "        raise ValueError(f\"Submission feature vector length {len(submission_features)} does not match expected {expected_length}.\")\n",
    "    \n",
    "    # Normalize using the training mean and std (stored as torch tensors).\n",
    "    submission_tensor = torch.tensor(submission_features, dtype=torch.float32).to(device)\n",
    "    submission_tensor = (submission_tensor - dataset.mean.to(device)) / dataset.std.to(device)\n",
    "    submission_tensor = submission_tensor.unsqueeze(0)  # Add batch dimension.\n",
    "    \n",
    "    # Predict using the model.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(submission_tensor)\n",
    "        prob = torch.sigmoid(logits).item()  # Convert logits to probability.\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(submission_file_path, model, dataset_input, lambda_decay=0.5, device='cpu'):\n",
    "    \"\"\"\n",
    "    Updates a Kaggle-style submission file by computing match predictions using a trained model.\n",
    "\n",
    "    This function reads a submission file containing match-ups identified by a submission key \n",
    "    (formatted as \"year_teamidA_teamidB\"), computes the aggregated statistics for teams using \n",
    "    `compute_aggs`, predicts the probability of team A winning against team B, and updates the \n",
    "    \"Pred\" column in the submission file with the predicted probabilities.\n",
    "\n",
    "    Args:\n",
    "        submission_file_path (str): Path to the CSV file containing the match submission entries.\n",
    "        model (torch.nn.Module): The trained PyTorch model used for making predictions.\n",
    "        dataset_input (str or CSVDataset): Either a file path to a CSV dataset or an instance of `CSVDataset`.\n",
    "        lambda_decay (float, optional): Decay factor for sample weighting when computing aggregates. Defaults to 0.5.\n",
    "        device (str, optional): The device ('cpu' or 'cuda') on which the model should run. Defaults to 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        None: The function updates the submission file in-place and writes the modified file to disk.\n",
    "\n",
    "    Side Effects:\n",
    "        - Reads the dataset from a file if `dataset_input` is a string.\n",
    "        - Computes aggregated team statistics and stores them in a CSV file (`input_data.csv`).\n",
    "        - Reads the submission file, updates the \"Pred\" column with probabilities, and writes the modified file.\n",
    "    \"\"\"\n",
    "    output_file_path = submission_file_path\n",
    "\n",
    "    # If dataset_input is a string, create a CSVDataset object from it.\n",
    "    if isinstance(dataset_input, str):\n",
    "        dataset = CSVDataset(dataset_input, lambda_decay=lambda_decay)\n",
    "    else:\n",
    "        dataset = dataset_input\n",
    "\n",
    "    # Compute the aggregate dictionaries (current and opponent) and create the CSV file.\n",
    "    current_aggs, opponent_aggs = compute_aggs(dataset, output_file_path='input_data.csv')\n",
    "    \n",
    "    # Read the sample submission CSV file.\n",
    "    sub_df = pd.read_csv(submission_file_path)\n",
    "    \n",
    "    # Loop over each submission key, predict the probability, and update the \"Pred\" column.\n",
    "    predictions = []\n",
    "    for idx, row in sub_df.iterrows():\n",
    "        submission_key = row['ID']\n",
    "        prob = predict_with_aggs(submission_key, model, current_aggs, opponent_aggs, dataset, device=device)\n",
    "        predictions.append(prob)\n",
    "    \n",
    "    sub_df['Pred'] = predictions\n",
    "    sub_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Submission file written to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/advaithvecham/Studying Stuff/PoC/march-machine-learning-mania-2025/MNCAATourneyDetailedResults.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_loader, test_loader, n_inputs = \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/Users/advaithvecham/Studying Stuff/PoC/march-machine-learning-mania-2025/MNCAATourneyDetailedResults.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_data_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m n_inputs = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_loader))[\u001b[32m0\u001b[39m].shape[\u001b[32m1\u001b[39m]\n\u001b[32m      4\u001b[39m model, loss_history = train_model(train_loader, test_loader, \n\u001b[32m      5\u001b[39m                 n_inputs,  \n\u001b[32m      6\u001b[39m                 dropout_value=\u001b[32m0.2\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m                 beta_2=\u001b[32m0.999\u001b[39m, \n\u001b[32m     15\u001b[39m                 verbose=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mprepare_data\u001b[39m\u001b[34m(path, test_size, batch_size, get_data_headers)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprepare_data\u001b[39m(path, test_size=\u001b[32m0.2\u001b[39m, batch_size=\u001b[32m64\u001b[39m, get_data_headers = \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    Loads a dataset from a CSV file, splits it into training and test sets, and returns PyTorch DataLoaders.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m \u001b[33;03m            - num_features (int): Number of features in the dataset.\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     data = \u001b[43mCSVDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m(get_data_headers):\n\u001b[32m     27\u001b[39m         \u001b[38;5;28mprint\u001b[39m(data.get_column_names())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 169\u001b[39m, in \u001b[36mCSVDataset.__init__\u001b[39m\u001b[34m(self, path, lambda_decay)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, lambda_decay=\u001b[32m0.5\u001b[39m):\n\u001b[32m    156\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[33;03m    Initializes the dataset by loading, processing, and normalizing match data.\u001b[39;00m\n\u001b[32m    158\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m \u001b[33;03m            on season recency. Defaults to 0.5.\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m     most_recent_season = df[\u001b[33m'\u001b[39m\u001b[33mSeason\u001b[39m\u001b[33m'\u001b[39m].max()\n\u001b[32m    171\u001b[39m     df = \u001b[38;5;28mself\u001b[39m.flip_data(df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Studying Stuff/completed_projects/basketball_predictor/.venv-1/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Studying Stuff/completed_projects/basketball_predictor/.venv-1/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Studying Stuff/completed_projects/basketball_predictor/.venv-1/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Studying Stuff/completed_projects/basketball_predictor/.venv-1/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Studying Stuff/completed_projects/basketball_predictor/.venv-1/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/advaithvecham/Studying Stuff/PoC/march-machine-learning-mania-2025/MNCAATourneyDetailedResults.csv'"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader, n_inputs = prepare_data(\"/Users/advaithvecham/Studying Stuff/PoC/march-machine-learning-mania-2025/MNCAATourneyDetailedResults.csv\", test_size=0.2, batch_size=64, get_data_headers=True)\n",
    "\n",
    "n_inputs = next(iter(train_loader))[0].shape[1]\n",
    "model, loss_history = train_model(train_loader, test_loader, \n",
    "                n_inputs,  \n",
    "                dropout_value=0.2,\n",
    "                hidden_units=[20, 10, 8, 1],\n",
    "                relu_slope=0.1,\n",
    "                learning_rate=0.001, \n",
    "                epochs=120, \n",
    "                optimizer_type='adam', \n",
    "                loss_fn ='bce', \n",
    "                beta_1=0.9, \n",
    "                beta_2=0.999, \n",
    "                verbose=True)\n",
    "print(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m plt.plot(\u001b[43mloss_history\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m],)\n\u001b[32m      2\u001b[39m plt.plot(loss_history[\u001b[33m'\u001b[39m\u001b[33mtest_loss\u001b[39m\u001b[33m'\u001b[39m],)\n",
      "\u001b[31mNameError\u001b[39m: name 'loss_history' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(loss_history['train_loss'],)\n",
    "plt.plot(loss_history['test_loss'],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated features written to input_data.csv\n",
      "Submission file written to /Users/advaithvecham/Studying Stuff/PoC/march-machine-learning-mania-2025/SampleSubmissionStage2.csv\n"
     ]
    }
   ],
   "source": [
    "dataset = CSVDataset(\"/Users/advaithvecham/Studying Stuff/PoC/march-machine-learning-mania-2025/MNCAATourneyDetailedResults.csv\", lambda_decay=0.5)\n",
    "# predict(\"/Users/advaithvecham/Studying Stuff/PoC/march-machine-learning-mania-2025/SampleSubmissionStage1.csv\", model, dataset, lambda_decay=0.5, device='cpu')\n",
    "predict(\"/Users/advaithvecham/Studying Stuff/PoC/march-machine-learning-mania-2025/SampleSubmissionStage2.csv\", model, dataset, lambda_decay=0.5, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-1 (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
